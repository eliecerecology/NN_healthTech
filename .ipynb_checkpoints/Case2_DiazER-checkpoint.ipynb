{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEAM 10: Eliecer Diaz\n",
    "\n",
    "AIM: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset of validated Chest X-Ray images described and analyzed in \"Deep learning-based classification and referral of treatable human diseases\".\n",
    "The data was split into three subsets: - 60 % train - 20 % validation - 20 % test\n",
    "Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), “Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification”, Mendeley Data, v2\n",
    "Version 2, published 2018-01-06 University of California San Diego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import collections\n",
    "random.seed(7)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.metrics import SensitivityAtSpecificity\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import tensorflow.keras.backend as k\n",
    "import tensorflow.keras.callbacks as Callback\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "Found GPU at: \n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "print(\"Found GPU at: {}\".format(device_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'/home/elvi/Desktop/pneumonia2/train' #/kaggle/input/pneumonia/pneumonia2/train'\n",
    "val_dir = r'/home/elvi/Desktop/pneumonia2/validation'#/kaggle/input/pneumonia/pneumonia2/validation'\n",
    "test_dir = r'/home/elvi/Desktop/pneumonia2/test' #/kaggle/input/pneumonia/pneumonia2/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PNEUMONIA', 'NORMAL']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1171 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        #verbose = 0,\n",
    "        shuffle=False,\n",
    "        target_size = (150, 150),\n",
    "        batch_size = 1, #<----tensorflow documentation\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_(SS, batch): # SS: number neurons in the last dense layer, batch: batch size \n",
    "    \n",
    "    #getting new metrics to calculate F1 at the end of the notebook\n",
    "    Preci = tf.keras.metrics.Precision() \n",
    "    Recal = tf.keras.metrics.Recall() \n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       rotation_range = 5, \n",
    "                                       horizontal_flip = True)\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        #verbose = 0,\n",
    "        target_size = (150, 150),\n",
    "        batch_size = batch,\n",
    "        class_mode = 'binary')\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        #verbose = 0,\n",
    "        target_size = (150, 150),\n",
    "        batch_size = batch,\n",
    "        class_mode='binary')\n",
    "    \n",
    "    try:\n",
    "        with tf.device('/device:GPU:0'):\n",
    "        #with tpu_strategy.scope():\n",
    "            model = tf.keras.models.Sequential()\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 3)))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "            model.add(Dense(SS, activation = 'relu'))\n",
    "            model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "            model.compile(loss = 'binary_crossentropy',\n",
    "                          optimizer = tf.keras.optimizers.RMSprop(lr = 1e-4),\n",
    "                          metrics = ['acc', Preci, Recal])\n",
    "            print(\"GPU/TPU IS ON\")\n",
    "    except:\n",
    "        \n",
    "            model = tf.keras.models.Sequential()\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 3)))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "            model.add(Dense(SS, activation = 'relu'))\n",
    "            model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "            model.compile(loss = 'binary_crossentropy',\n",
    "                          optimizer = tf.keras.optimizers.RMSprop(lr = 1e-4),\n",
    "                          metrics = ['acc', Preci, Recal])\n",
    "            print(\"NOT USING GPU\")\n",
    "\n",
    "    return model, train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n",
      "Found 3513 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n",
      "NOT USING GPU\n"
     ]
    }
   ],
   "source": [
    "SS = [256] # we divide them by 10 after to get: 0.5, 0.7, 0.9\n",
    "batches = [8, 32, 64]\n",
    "models = {}\n",
    "train_generators = {}\n",
    "val_generators = {}\n",
    "\n",
    "for ss in SS:\n",
    "    for batch in batches:\n",
    "        model_ss_batch = 'models_{}_{}'.format(ss, batch)  # creating the keys\n",
    "        models[model_ss_batch] = model_(ss//10, batch) # appending the model to the key\n",
    "        \n",
    "        train_generator_ss_batch = 'train_generators_{}_{}'.format(ss, batch)  # creating the keys\n",
    "        train_generators[train_generator_ss_batch] = model_(ss//10, batch) # appending the generator to the key\n",
    "        \n",
    "        val_generator_ss_batch = 'val_generators_{}_{}'.format(ss, batch)  # creating the keys\n",
    "        val_generators[val_generator_ss_batch] = model_(ss//10, batch) # appending the model to the key       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['models_256_8', 'models_256_32', 'models_256_64'])\n",
      "dict_keys(['train_generators_256_8', 'train_generators_256_32', 'train_generators_256_64'])\n",
      "dict_keys(['val_generators_256_8', 'val_generators_256_32', 'val_generators_256_64'])\n",
      "1171\n"
     ]
    }
   ],
   "source": [
    "print(models.keys()) #prints keys\n",
    "print(train_generators.keys()) #prints keys\n",
    "print(val_generators.keys()) #prints keys\n",
    "\n",
    "print(test_generator.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-abb69bf56f96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# = 1172 / 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# saving the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Final_models = {}\n",
    "for i in range(len(models)):\n",
    "    print('MODEL:',i)\n",
    "    Final_model_i = 'models_{}'.format(i)  # creating the keys\n",
    "    Final_models[Final_model_i] = models[list(models.keys())[i]][0].fit_generator(\n",
    "        train_generators[list(train_generators.keys())[i]][1],\n",
    "        steps_per_epoch = None, # = 3513 / 8\n",
    "        verbose = 0,\n",
    "        epochs = 100,\n",
    "        validation_data = val_generators[list(val_generators.keys())[i]][1],\n",
    "        validation_steps = None # = 1172 / 8\n",
    "    )\n",
    "    # saving the model\n",
    "    models[list(models.keys())[i]][0].save(\"model{}.h5\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Final_models)):\n",
    "    print(i)\n",
    "    models_key = Final_models.keys()\n",
    "    model_key = [j  for  j in  models_key]\n",
    "    metrics = Final_models[list(Final_models.keys())[i]].history.keys()\n",
    "    metric = [k  for  k in  metrics]\n",
    "    \n",
    "    acc = Final_models[list(Final_models.keys())[i]].history[metric[1]]\n",
    "    val_acc = Final_models[list(Final_models.keys())[i]].history[metric[5]]\n",
    "    \n",
    "    preci = Final_models[list(Final_models.keys())[i]].history[metric[2]]\n",
    "    val_preci = Final_models[list(Final_models.keys())[i]].history[metric[6]]\n",
    "    \n",
    "    recal = Final_models[list(Final_models.keys())[i]].history[metric[3]]\n",
    "    val_recal = Final_models[list(Final_models.keys())[i]].history[metric[7]]\n",
    "    \n",
    "    loss = Final_models[list(Final_models.keys())[i]].history[metric[0]]\n",
    "    val_loss = Final_models[list(Final_models.keys())[i]].history[metric[4]]\n",
    "    \n",
    "    epochs = range(len(acc))\n",
    "        \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 10))\n",
    "    \n",
    "    \n",
    "    axs[ 0].set_title('1. Accuracy')\n",
    "    axs[ 0].plot(epochs, acc, 'bo-', label = 'Training acc')\n",
    "    axs[ 0].plot(epochs, val_acc, 'r*-', label = 'Validation acc')\n",
    "    \n",
    "    axs[ 1].set_title('2. Precision ')\n",
    "    axs[ 1].plot(epochs, preci, 'bo-', label = 'Precision_training ')\n",
    "    axs[ 1].plot(epochs, val_preci, 'r*-', label = 'Precision validation')\n",
    "    \n",
    "    axs[ 2].set_title('3. Recall (sensitivity) ')\n",
    "    axs[ 2].plot(epochs, recal, 'bo-', label = 'Recall training')\n",
    "    axs[ 2].plot(epochs, val_recal, 'r*-', label = 'Recall validation')\n",
    "        \n",
    "    axs[ 3].set_title('4. Loss')\n",
    "    axs[ 3].plot(epochs, loss, 'bo-', label = 'Training loss')\n",
    "    axs[ 3].plot(epochs, val_loss, 'r*-', label = 'Validation loss')\n",
    "    \n",
    "      \n",
    "    fig.suptitle(model_key[i], fontname=\"Times New Roman\",fontweight=\"bold\")\n",
    "    fig.text(0.5, 0.04, 'EPOCH', ha='center', fontname=\"Times New Roman\",fontweight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "0.2110902935266495 0.903125 0.8972332 0.9784483\n"
     ]
    }
   ],
   "source": [
    "# RESULTS? YES...\n",
    "test_generator.reset() # resetting generator\n",
    "for i in range(0,len(Final_models)):\n",
    "    Loss, Accuracy, Preci, Recal = models[list(models.keys())[i]][0].evaluate_generator(generator=val_generators[list(val_generators.keys())[i]][1], steps=10)\n",
    "    print(\"Model\",i)\n",
    "    print('Loss: {}'.format(Loss), 'Accuracy: {}'.format(Accuracy), 'Precision: {}'.format(Preci), 'Recall: {}'.format(Recal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 0\n",
      "Confusion Matrix\n",
      "[[233  84]\n",
      " [ 13 841]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.74      0.83       317\n",
      "           0       0.91      0.98      0.95       854\n",
      "\n",
      "    accuracy                           0.92      1171\n",
      "   macro avg       0.93      0.86      0.89      1171\n",
      "weighted avg       0.92      0.92      0.91      1171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get actuall y_labels from test set\n",
    "Y_labels = test_generator.classes\n",
    "\n",
    "# E.g. using model 2, make some predictions with cut-off 0.5\n",
    "for i in range(0,len(Final_models)):\n",
    "    print(\"Prediction for model {}:\".format(i))\n",
    "    \n",
    "    Y_pred = models[list(models.keys())[i]][0].predict_generator(test_generator)\n",
    "    Y_pred = 1*(Y_pred.astype('float64') > 0.5)\n",
    "    \n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(Y_labels, Y_pred))\n",
    "    print('Classification Report')\n",
    "    target_names = ['1','0']\n",
    "    print(classification_report(Y_labels, Y_pred, target_names=target_names))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(Y_labels, Y_pred)\n",
    "#logit_roc_auc_small = roc_auc_score(y_test, model.predict(X_test))\n",
    "#logit_roc_auc_small_reg1 = roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_labels, Y_pred)\n",
    "#fpr_small, tpr_small, thresholds_small = roc_curve(y_test, model_small.predict_proba(X_test))\n",
    "#fpr_small_reg1, tpr_small_reg1, thresholds_small_reg1 = roc_curve(y_test, model_small_reg1.predict_proba(X_test))\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='MODEL 1 (area = %0.2f)' % logit_roc_auc)\n",
    "#plt.plot(fpr_small, tpr_small, label='MODEL SMALL (area = %0.2f)' % logit_roc_auc_small)\n",
    "#plt.plot(fpr_small_reg1, tpr_small_reg1, label='MODEL SMALL + REGUL2 (area = %0.2f)' % logit_roc_auc_small_reg1)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What for??\n",
    "#sensitivity at specificity? sensitivity \n",
    "Y_labels = Y_labels.reshape(1171,)\n",
    "Y_pred = Y_pred.reshape(1171,)\n",
    "m = tf.keras.metrics.SensitivityAtSpecificity(0.9, num_thresholds = 1)\n",
    "m.update_state(Y_labels, Y_pred)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
